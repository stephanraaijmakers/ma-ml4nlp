{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba69bc9",
   "metadata": {},
   "source": [
    "# Local LLM with Sentence Transformers (Colab-ready)\n",
    "\n",
    "This notebook loads a small local LLM (e.g. Gemma) and evaluates its outputs with sentence-transformer embeddings.\n",
    "\n",
    "On Google Colab, run the next cell once to install required libraries, then run the code cells in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d24d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stephan Raaijmakers 2025\n",
    "# Setup and imports\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from typing import List, Tuple\n",
    " # Stephan Raaijmakers, 2025\n",
    "\n",
    "# pip install sentence_transformers\n",
    "# Sample run (script version, outside Colab):\n",
    "# python local_llm_sentence_transformers.py --model google/gemma-2-2b-it --input_file input.txt --max_new_tokens 30\n",
    "\n",
    "# Sample input.txt:\n",
    "# -------------------\n",
    "# Prompt: What is an empathetic version of: I don't care about that. Respond with only 1 example (a single sentence).\n",
    "# I'm sorry but I have little affinity with that\n",
    "# Prompt: What is a sarcastic version of: I care about that. Respond with only 1 example (a single sentence).\n",
    "# I don't care at all\n",
    "\n",
    "# You can try out prompt designs like this:\n",
    "# python local_llm_sentence_transformers.py --model google/gemma-2-2b-it --question \"What is an empathetic version of: I don't care about that. Respond with just one sentence (a rephrase)\" --reference \"I'm sorry but I have little affinity with that\" --max_new_tokens 20\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once on Colab)\n",
    "\n",
    "!pip install -q sentence-transformers transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894741c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name_or_path: str) -> Tuple[AutoModelForCausalLM, AutoTokenizer, torch.device]:\n",
    "    \"\"\"Load a (small) causal LLM such as Gemma locally using transformers.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    print(f\"Loading model '{model_name_or_path}' on device: {device}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "    # Use float16 on GPU, float32 on CPU for compatibility.\n",
    "    torch_dtype = torch.float16 if device.type == \"cuda\" else torch.float32\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "\n",
    "def generate_answer(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    device: torch.device,\n",
    "    question: str,\n",
    "    max_new_tokens: int = 128,\n",
    ") -> str:\n",
    "    \"\"\"Generate an answer from the local LLM for the given question.\"\"\"\n",
    "    # Very simple prompt format; adapt if you want a different style.\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode the full sequence and strip the prompt part if possible.\n",
    "    full_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    # Try to cut off the prompt prefix if it matches.\n",
    "    if full_text.startswith(prompt):\n",
    "        answer = full_text[len(prompt) :].strip()\n",
    "    else:\n",
    "        answer = full_text.strip()\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a708bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_embeddings(reference: str, hypothesis: str, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\") -> float:\n",
    "    \"\"\"Compute cosine similarity between reference and hypothesis sentence embeddings.\"\"\"\n",
    "    embed_model = SentenceTransformer(model_name)\n",
    "    embeddings = embed_model.encode([reference, hypothesis])\n",
    "    ref_emb, hyp_emb = embeddings[0], embeddings[1]\n",
    "    # Cosine similarity\n",
    "    num = float(np.dot(ref_emb, hyp_emb))\n",
    "    den = float(np.linalg.norm(ref_emb) * np.linalg.norm(hyp_emb))\n",
    "    if den == 0.0:\n",
    "        return 0.0\n",
    "    return num / den\n",
    "\n",
    "\n",
    "def read_prompt_reference_pairs(path: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Read blocks of two lines: 'Prompt: ...' and reference answer.\n",
    "\n",
    "    Expected format per example (no blank lines required):\n",
    "      Prompt: <question text>\n",
    "      <reference answer>\n",
    "    \"\"\"\n",
    "    pairs: List[Tuple[str, str]] = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.rstrip(\"\\n\") for line in f]\n",
    "\n",
    "    i = 0\n",
    "    while i + 1 < len(lines):\n",
    "        prompt_line = lines[i].strip()\n",
    "        ref_line = lines[i + 1].strip()\n",
    "        if not prompt_line.startswith(\"Prompt:\"):\n",
    "            raise ValueError(f\"Expected line starting with 'Prompt:' at line {i+1}, got: {prompt_line!r}\")\n",
    "        question = prompt_line[len(\"Prompt:\") :].strip()\n",
    "        reference = ref_line\n",
    "        pairs.append((question, reference))\n",
    "        i += 2\n",
    "\n",
    "    if i < len(lines):\n",
    "        # Odd number of lines -> last line has no pair\n",
    "        raise ValueError(\"Input file has an odd number of lines; each example must have exactly two lines.\")\n",
    "\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=(\n",
    "            \"Load a local LLM (e.g., Gemma), generate an answer for a question, \",\n",
    "            \"and evaluate it against a reference answer using cosine similarity on sentence embeddings.\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        default=\"google/gemma-2-2b-it\",\n",
    "        help=(\n",
    "            \"Hugging Face model ID or local path to a causal LLM. \",\n",
    "            \"Default is a small Gemma instruction-tuned model.\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--input_file\",\n",
    "        type=str,\n",
    "        required=False,\n",
    "        help=(\n",
    "            \"Optional path to a file containing multiple examples as two-line blocks: \",\n",
    "            \"'Prompt: <text>' on the first line and the reference answer on the second line. \",\n",
    "            \"If not provided, you must specify --question and --reference.\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--question\",\n",
    "        type=str,\n",
    "        required=False,\n",
    "        help=\"Input question / prompt to ask the model (single-example mode).\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--reference\",\n",
    "        type=str,\n",
    "        required=False,\n",
    "        help=\"Reference (gold) answer string for evaluation (single-example mode).\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--max_new_tokens\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"Maximum number of new tokens to generate for the answer.\",\n",
    "    )\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    args = parse_args()\n",
    "\n",
    "    model, tokenizer, device = load_model_and_tokenizer(args.model)\n",
    "\n",
    "    # If an input file is provided, process all examples and write predictions.\n",
    "    if args.input_file:\n",
    "        pairs = read_prompt_reference_pairs(args.input_file)\n",
    "        generic_out_path = os.path.join(os.path.dirname(args.input_file) or \".\", \"predictions.txt\")\n",
    "\n",
    "        print(f\"Processing {len(pairs)} examples from {args.input_file} ...\")\n",
    "        print(f\"Writing predictions to {generic_out_path}\")\n",
    "\n",
    "        with open(generic_out_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "            for idx, (question, reference) in enumerate(pairs, start=1):\n",
    "                hypothesis = generate_answer(\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    device=device,\n",
    "                    question=question,\n",
    "                    max_new_tokens=args.max_new_tokens,\n",
    "                )\n",
    "\n",
    "                cosine_sim = cosine_similarity_embeddings(reference=reference, hypothesis=hypothesis)\n",
    "\n",
    "                out_f.write(f\"Example {idx}\\n\")\n",
    "                out_f.write(f\"Prompt: {question}\\n\")\n",
    "                out_f.write(f\"Reference: {reference}\\n\")\n",
    "                out_f.write(f\"Answer: {hypothesis}\\n\")\n",
    "                out_f.write(f\"CosineSimilarity: {cosine_sim:.4f}\\n\")\n",
    "                out_f.write(\"\\n\")\n",
    "\n",
    "        print(\"Done.\")\n",
    "\n",
    "    else:\n",
    "        # Single example mode: require question and reference.\n",
    "        if not args.question or not args.reference:\n",
    "            raise SystemExit(\"Either provide --input_file or both --question and --reference.\")\n",
    "\n",
    "        print(\"\\n=== QUESTION ===\")\n",
    "        print(args.question)\n",
    "\n",
    "        print(\"\\n=== REFERENCE ANSWER ===\")\n",
    "        print(args.reference)\n",
    "\n",
    "        print(\"\\nGenerating model answer...\\n\")\n",
    "        hypothesis = generate_answer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=device,\n",
    "            question=args.question,\n",
    "            max_new_tokens=args.max_new_tokens,\n",
    "        )\n",
    "\n",
    "        print(\"=== MODEL ANSWER ===\")\n",
    "        print(hypothesis)\n",
    "\n",
    "        print(\"\\nComputing cosine similarity between reference and model answer (sentence embeddings)...\\n\")\n",
    "        cosine_sim = cosine_similarity_embeddings(reference=args.reference, hypothesis=hypothesis)\n",
    "\n",
    "        print(\"=== COSINE SIMILARITY (SENTENCE EMBEDDINGS) ===\")\n",
    "        print(f\"cosine_similarity: {cosine_sim:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# In a notebook / Colab environment we typically use the\n",
    "# functions above directly instead of relying on a CLI entrypoint.\n",
    "# To run this as a script outside Colab, you could add:\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea42119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: interactive use in Colab / notebook\n",
    "\n",
    "question = \"What is an empathetic version of: I don't care about that.\"\n",
    "reference = \"I'm sorry but I have little affinity with that\"\n",
    "\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "\n",
    "print(\"Loading model... (this may take a while)\")\n",
    "model, tokenizer, device = load_model_and_tokenizer(model_name)\n",
    "\n",
    "print(\"\\nGenerating answer...\\n\")\n",
    "hypothesis = generate_answer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    question=question,\n",
    "    max_new_tokens=64,\n",
    " )\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Reference:\", reference)\n",
    "print(\"\\nModel answer:\", hypothesis)\n",
    "\n",
    "print(\"\\nCosine similarity between reference and answer:\")\n",
    "cos_sim = cosine_similarity_embeddings(reference=reference, hypothesis=hypothesis)\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a78c4f9",
   "metadata": {},
   "source": [
    "## Using a local input file\n",
    "\n",
    "The helper `read_prompt_reference_pairs(path)` expects a text file with blocks of two lines:\n",
    "\n",
    "```text\n",
    "Prompt: <question text>\n",
    "<reference answer>\n",
    "Prompt: <next question>\n",
    "<next reference>\n",
    "...\n",
    "```\n",
    "\n",
    "In Colab you can either:\n",
    "- Upload a local file using `files.upload()` (see next cell), or\n",
    "- Use a path that already exists in the Colab filesystem (e.g. in Drive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d36bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: load a local input file and process all examples\n",
    "\n",
    "# Option 1 (Colab): upload a file from your computer\n",
    "try:\n",
    "    from google.colab import files  # type: ignore\n",
    "    uploaded = files.upload()  # Choose your input.txt file in the dialog\n",
    "    input_path = list(uploaded.keys())[0]\n",
    "    print(\"Uploaded file:\", input_path)\n",
    "except ImportError:\n",
    "    # Not running in Colab; set an existing local path instead.\n",
    "    input_path = \"input.txt\"  # adapt this path as needed\n",
    "    print(\"Using local path:\", input_path)\n",
    "\n",
    "# Load all (question, reference) pairs from the file\n",
    "pairs = read_prompt_reference_pairs(input_path)\n",
    "print(f\"Loaded {len(pairs)} examples from {input_path}\")\n",
    "\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "print(\"\\nLoading model... (this may take a while)\")\n",
    "model, tokenizer, device = load_model_and_tokenizer(model_name)\n",
    "\n",
    "for idx, (question, reference) in enumerate(pairs, start=1):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(\"Prompt:\", question)\n",
    "    print(\"Reference:\", reference)\n",
    "\n",
    "    hypothesis = generate_answer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        question=question,\n",
    "        max_new_tokens=64,\n",
    "    )\n",
    "\n",
    "    print(\"Model answer:\", hypothesis)\n",
    "    cos_sim = cosine_similarity_embeddings(reference=reference, hypothesis=hypothesis)\n",
    "    print(\"Cosine similarity:\", cos_sim)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
