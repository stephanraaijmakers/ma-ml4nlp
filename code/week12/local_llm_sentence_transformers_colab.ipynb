{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e99713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (Colab)\n",
    "!pip install -q transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d2ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name_or_path: str):\n",
    "    \"\"\"Load a (small) causal LLM such as Gemma locally using transformers.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    print(f\"Loading model '{model_name_or_path}' on device: {device}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "    # Use float16 on GPU, float32 on CPU for compatibility.\n",
    "    torch_dtype = torch.float16 if device.type == \"cuda\" else torch.float32\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "\n",
    "def generate_answer(model, tokenizer, device, question: str, max_new_tokens: int = 128) -> str:\n",
    "    \"\"\"Generate an answer from the local LLM for the given question.\"\"\"\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    full_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    if full_text.startswith(prompt):\n",
    "        answer = full_text[len(prompt):].strip()\n",
    "    else:\n",
    "        answer = full_text.strip()\n",
    "    return answer\n",
    "\n",
    "\n",
    "def cosine_similarity_embeddings(reference: str, hypothesis: str, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\") -> float:\n",
    "    \"\"\"Compute cosine similarity between reference and hypothesis sentence embeddings.\"\"\"\n",
    "    embed_model = SentenceTransformer(model_name)\n",
    "    embeddings = embed_model.encode([reference, hypothesis])\n",
    "    ref_emb, hyp_emb = embeddings[0], embeddings[1]\n",
    "    num = float(np.dot(ref_emb, hyp_emb))\n",
    "    den = float(np.linalg.norm(ref_emb) * np.linalg.norm(hyp_emb))\n",
    "    if den == 0.0:\n",
    "        return 0.0\n",
    "    return num / den\n",
    "\n",
    "\n",
    "def read_prompt_reference_pairs(path: str):\n",
    "    \"\"\"Read blocks of two lines: 'Prompt: ...' and reference answer.\n",
    "\n",
    "    Expected format per example (no blank lines required):\n",
    "      Prompt: <question text>\n",
    "      <reference answer>\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.rstrip(\"\\n\") for line in f]\n",
    "\n",
    "    i = 0\n",
    "    while i + 1 < len(lines):\n",
    "        prompt_line = lines[i].strip()\n",
    "        ref_line = lines[i + 1].strip()\n",
    "        if not prompt_line.startswith(\"Prompt:\"):\n",
    "            raise ValueError(f\"Expected line starting with 'Prompt:' at line {i+1}, got: {prompt_line!r}\")\n",
    "        question = prompt_line[len(\"Prompt:\"):].strip()\n",
    "        reference = ref_line\n",
    "        pairs.append((question, reference))\n",
    "        i += 2\n",
    "\n",
    "    if i < len(lines):\n",
    "        raise ValueError(\"Input file has an odd number of lines; each example must have exactly two lines.\")\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e36d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload an input file with two-line blocks (Prompt / Reference)\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "input_filename = list(uploaded.keys())[0]\n",
    "print('Uploaded file:', input_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model and generation settings\n",
    "model_name = 'google/gemma-2-2b-it'  # change if needed\n",
    "max_new_tokens = 128\n",
    "\n",
    "model, tokenizer, device = load_model_and_tokenizer(model_name)\n",
    "pairs = read_prompt_reference_pairs(input_filename)\n",
    "print(f'Loaded {len(pairs)} examples from', input_filename)\n",
    "\n",
    "output_path = 'predictions.txt'\n",
    "with open(output_path, 'w', encoding='utf-8') as out_f:\n",
    "    for idx, (question, reference) in enumerate(pairs, start=1):\n",
    "        hypothesis = generate_answer(model, tokenizer, device, question, max_new_tokens=max_new_tokens)\n",
    "        cosine_sim = cosine_similarity_embeddings(reference=reference, hypothesis=hypothesis)\n",
    "        out_f.write(f'Example {idx}\\n')\n",
    "        out_f.write(f'Prompt: {question}\\n')\n",
    "        out_f.write(f'Reference: {reference}\\n')\n",
    "        out_f.write(f'Answer: {hypothesis}\\n')\n",
    "        out_f.write(f'CosineSimilarity: {cosine_sim:.4f}\\n')\n",
    "        out_f.write('\\n')\n",
    "\n",
    "print('Wrote predictions to', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the predictions file to your local machine\n",
    "from google.colab import files\n",
    "files.download('predictions.txt')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
